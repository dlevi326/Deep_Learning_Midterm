{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvLoader(object):\n",
    "    def __init__(self,filename):\n",
    "        self.file = open(filename, mode='r')\n",
    "\n",
    "    def tokenizeCsv(self,sentences):\n",
    "        t = Tokenizer()\n",
    "        t.fit_on_texts(sentences)\n",
    "        #encoded_words = t.texts_to_sequences(sentences)\n",
    "        return t\n",
    "\n",
    "    def getCodedWords(self):\n",
    "        csv_reader = csv.DictReader(self.file,fieldnames=['Category','Title','Summary'])\n",
    "        ignore_words = set(stopwords.words('english'))\n",
    "        manual_stops = ['(',')']\n",
    "        ignore_words = list(ignore_words)+list(set(manual_stops))\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for row in csv_reader:\n",
    "            words = text_to_word_sequence(row['Summary'])\n",
    "            sentence = \"\"\n",
    "            for w in words:\n",
    "                sentence+=w+\" \"\n",
    "            sentences.append(sentence);\n",
    "\n",
    "        return self.tokenizeCsv(sentences), sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvTrain = CsvLoader('./smallTrain.csv')\n",
    "token, docs = csvTrain.getCodedWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"reuters short sellers wall street's dwindling band of ultra cynics are seeing green again \", 'reuters private investment firm carlyle group which has a reputation for making well timed and occasionally controversial plays in the defense industry has quietly placed its bets on another part of the market ', 'reuters soaring crude prices plus worries about the economy and the outlook for earnings are expected to hang over the stock market next week during the depth of the summer doldrums ', 'reuters authorities have halted oil export flows from the main pipeline in southern iraq after intelligence showed a rebel militia could strike infrastructure an oil official said on saturday ']\n"
     ]
    }
   ],
   "source": [
    "print(docs[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenizer\n",
    "t = Tokenizer()\n",
    "# Create unique indexes for words\n",
    "t.fit_on_texts(docs)\n",
    "# Vocab size\n",
    "vSize = len(t.word_index) + 1\n",
    "# Turn each document into a bunch of indexes that map to words\n",
    "encoded_docs = t.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 522, 1632, 400, 2182, 1633, 1003, 4, 2183, 2184, 22, 1004, 1005, 256], [14, 707, 1634, 357, 3362, 222, 68, 30, 2, 3363, 8, 596, 460, 3364, 6, 3365, 1267, 3366, 5, 1, 827, 223, 30, 3367, 1635, 19, 3368, 7, 401, 358, 4, 1, 127], [14, 828, 325, 87, 461, 326, 40, 1, 195, 6, 1, 708, 8, 462, 22, 327, 3, 1268, 46, 1, 196, 127, 107, 72, 168, 1, 1006, 4, 1, 463, 1269], [14, 235, 33, 3369, 59, 3370, 2185, 21, 1, 709, 3371, 5, 710, 208, 44, 829, 1007, 2, 3372, 2186, 42, 328, 1008, 17, 59, 329, 24, 7, 209]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_docs[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knobs\n",
    "MAX_SENTENCE_LENGTH = 20\n",
    "EMBED_INPUT_DIM = vSize\n",
    "EMBED_OUTPUT_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the docs to maintain input shape\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=MAX_SENTENCE_LENGTH, padding='post')\n",
    "\n",
    "# Create model\n",
    "model = keras.Sequential()\n",
    "# Embedding layer to learn embeddings with input size = Max sentence length\n",
    "model.add(keras.layers.Embedding(EMBED_INPUT_DIM,EMBED_OUTPUT_DIM,input_length=MAX_SENTENCE_LENGTH))\n",
    "# Train model\n",
    "model.compile('rmsprop','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn documents into a series of word embeddings using model\n",
    "def encode_docs(in_docs):\n",
    "    out = model.predict(in_docs)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning docs into word embeddings (Change to add docs)\n",
    "embedded_docs_by_words = encode_docs(padded_docs)\n",
    "doc_codes = {}\n",
    "for code,d in zip(embedded_docs_by_words,docs):\n",
    "    doc_codes[d] = code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01026853 -0.01127439  0.03110701  0.03032035  0.04276967  0.02096802\n",
      " -0.02921199  0.02517955 -0.02183627  0.03298967 -0.00423329  0.04074765\n",
      " -0.01566428 -0.04449521  0.00276446 -0.03780801 -0.04217818  0.00611117\n",
      " -0.01618359  0.03942638  0.02700111  0.03197521  0.0487999  -0.04115957\n",
      "  0.04092618 -0.00229242  0.0067974  -0.03362022 -0.00906729 -0.02366866\n",
      " -0.0288954   0.00842815]\n"
     ]
    }
   ],
   "source": [
    "# Printing one word's embedding for the document\n",
    "print(doc_codes[docs[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns entire document into 1xEMBED_OUTPUT_DIM so it can be compare to a query embedding\n",
    "def minimize_doc_embedding(doc_dict):\n",
    "    new_dict = {}\n",
    "    for key in doc_dict.keys():\n",
    "        newArr = []\n",
    "        for dim in range(EMBED_OUTPUT_DIM):\n",
    "            #newArr = []\n",
    "            sumArr = 0\n",
    "            for i in range(MAX_SENTENCE_LENGTH):\n",
    "                sumArr+=doc_dict[key][i][dim]\n",
    "                #print('Document:',key,'number col:',dim,'number row:',i,'=',doc_dict[key][i][dim])\n",
    "            newArr.append(float(sumArr)/float(MAX_SENTENCE_LENGTH))\n",
    "            #print('Document:',key,'number col:',dim,'=',float(sumArr)/float(MAX_SENTENCE_LENGTH))\n",
    "        new_dict[key] = newArr\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_doc_codes = minimize_doc_embedding(doc_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002910548448562622, 0.006346642877906561, 0.0018008933402597904, -0.001895968709141016, -0.0011425351724028588, 0.004518786631524563, -0.015242862701416015, 0.010690431483089924, 0.013549551274627447, 0.002522165235131979, 0.014493837021291256, 0.0167920027859509, -0.008100176695734262, 0.0021769657731056215, 0.0025395315140485765, -0.0002859458327293396, -0.016686940658837558, 0.007020661421120167, 0.004684101603925228, 0.006036215927451849, -0.010053078923374415, 0.009036272112280131, 0.010586696024984122, -0.009656254947185517, -0.007826289162039757, -0.00018333010375499726, 0.0060262314043939115, 0.008215367514640092, 0.006242496147751808, 0.006888321042060852, -0.022521489672362803, -0.006254567671567202]\n"
     ]
    }
   ],
   "source": [
    "print(min_doc_codes[docs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to turn query into a series of word indexes\n",
    "def indexQuery(model,que,t):\n",
    "    padded_queries = []\n",
    "    newQueries = []\n",
    "    for q in que:\n",
    "        words = text_to_word_sequence(q)\n",
    "        query=\"\"\n",
    "        for i,w in enumerate(words):\n",
    "            query+=w+\" \"\n",
    "        newQueries.append(query)\n",
    "    codeQuery = t.texts_to_sequences(newQueries)\n",
    "    padded_query = keras.preprocessing.sequence.pad_sequences(codeQuery, maxlen=MAX_SENTENCE_LENGTH, padding='post')\n",
    "    #padded_queries.append(padded_queries)\n",
    "    return padded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qList = [\"retail sales bounced back claims jobless benefits fell week economy is improving slump\"]\n",
    "# Embed query\n",
    "padQ = indexQuery(model,qList,t)\n",
    "coded_qs = model.predict(padQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of queries and codes\n",
    "q_dict = {}\n",
    "for q,code in zip(qList,coded_qs):\n",
    "    q_dict[q] = code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_query_embedding(q_dict):\n",
    "    new_dict = {}\n",
    "    for q in q_dict.keys():\n",
    "        newArr = []\n",
    "        for dim in range(EMBED_OUTPUT_DIM):\n",
    "            sumArr = 0\n",
    "            for i in range(MAX_SENTENCE_LENGTH):\n",
    "                sumArr+=q_dict[q][i][dim]\n",
    "            newArr.append(float(sumArr)/float(MAX_SENTENCE_LENGTH))\n",
    "            #print('Document:',key,'number col:',dim,'=',float(sumArr)/float(MAX_SENTENCE_LENGTH))\n",
    "        new_dict[q] = newArr\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query embedding\n",
    "min_q_dict = minimize_query_embedding(q_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the document most similar to the query\n",
    "def get_most_sim(q_sim_dict,d_sim_dict):\n",
    "    for q in q_sim_dict.keys():\n",
    "        min_dist = 100000000\n",
    "        min_d = \"None\"\n",
    "        for d in d_sim_dict.keys():\n",
    "            #temp = cosine_similarity((q_sim_dict[q]),(d_sim_dict[d]))\n",
    "            # Should switch to cosine sim\n",
    "            temp = abs(spatial.distance.cosine(q_sim_dict[q],d_sim_dict[d]))\n",
    "            if(temp<min_dist):\n",
    "                min_dist = temp\n",
    "                min_d = d\n",
    "            #print('Distance metric:',temp,'with document:',d)\n",
    "        print('Most similar to query:',q,\"is document:\",min_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to query: retail sales bounced back claims jobless benefits fell week economy is improving slump is document: celebrity fashion is booming these webpreneurs are bringing it to main street \n"
     ]
    }
   ],
   "source": [
    "get_most_sim(min_q_dict,min_doc_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
